{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Q2weXzB58RT"
   },
   "source": [
    "#**Install Library**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YT64KR7e5q99",
    "outputId": "83f03215-caf0-4975-b098-7fd718ba1bec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Sastrawi\n",
      "  Downloading Sastrawi-1.0.1-py2.py3-none-any.whl.metadata (909 bytes)\n",
      "Downloading Sastrawi-1.0.1-py2.py3-none-any.whl (209 kB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/209.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.7/209.7 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: Sastrawi\n",
      "Successfully installed Sastrawi-1.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install Sastrawi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7319lrMk6D7j"
   },
   "source": [
    "#**Case Folding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d4WsHKFw6oLV",
    "outputId": "a6332578-2125-4e76-b12d-51937aa69472"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teks Asli: Ini Adalah Contoh Teks Yang Akan Di Konvewrsi Menjadi Lowercase\n",
      "Teks Lowercase: ini adalah contoh teks yang akan di konvewrsi menjadi lowercase\n"
     ]
    }
   ],
   "source": [
    "#teks\n",
    "teks_asli = \"Ini Adalah Contoh Teks Yang Akan Di Konvewrsi Menjadi Lowercase\"\n",
    "\n",
    "# Mengubah Teks menjadI lOWERCASE\n",
    "teks_lower = teks_asli.lower()\n",
    "\n",
    "# Menampilkan Hasil\n",
    "print(\"Teks Asli:\", teks_asli)\n",
    "print(\"Teks Lowercase:\", teks_lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eqv3CX1X69ZM",
    "outputId": "94e073d5-a80e-4f58-fe21-712625d78878"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teks Asli:\n",
      "\n",
      "Ini adalah contoh teks dengan campuran huruf besar dan kecil.\n",
      "Contoh ini digunakan untuk demonstrasi case folding dalam pra-pemrosesan teks.\n",
      "Dengan menggunakan case folding, semua huruf dalam teks akan diubah menjadi huruf kecil.\n",
      "Ini membantu dalam memastikan konsistensi dalam analisis teks.\n",
      "\n",
      "\n",
      "Teks Lowercase dengan case folding:\n",
      "\n",
      "ini adalah contoh teks dengan campuran huruf besar dan kecil.\n",
      "contoh ini digunakan untuk demonstrasi case folding dalam pra-pemrosesan teks.\n",
      "dengan menggunakan case folding, semua huruf dalam teks akan diubah menjadi huruf kecil.\n",
      "ini membantu dalam memastikan konsistensi dalam analisis teks.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# teks dengan campuran huruf besar dan kecil\n",
    "teks_asli = \"\"\"\n",
    "Ini adalah contoh teks dengan campuran huruf besar dan kecil.\n",
    "Contoh ini digunakan untuk demonstrasi case folding dalam pra-pemrosesan teks.\n",
    "Dengan menggunakan case folding, semua huruf dalam teks akan diubah menjadi huruf kecil.\n",
    "Ini membantu dalam memastikan konsistensi dalam analisis teks.\n",
    "\"\"\"\n",
    "\n",
    "# Mengubah teks menjadi lowercase menggunakan case folding\n",
    "teks_lower = teks_asli.lower()\n",
    "\n",
    "# Menampilkan hasil\n",
    "print(\"Teks Asli:\")\n",
    "print(teks_asli)\n",
    "print(\"\\nTeks Lowercase dengan case folding:\")\n",
    "print(teks_lower)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GQw15T1s7nKh"
   },
   "source": [
    "# **A. Removal Special Karakter**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mJoF028b7vQi"
   },
   "source": [
    "## **A.1 Menghapus angka**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o-kI-g0370EW",
    "outputId": "8e989893-9774-447f-8763-1d8432078e43"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teks dengan angka:\n",
      "Ini adalah contoh teks dengan angka 12345 yang akan dihapus.\n",
      "\n",
      "Teks tanpa angka:\n",
      "Ini adalah contoh teks dengan angka  yang akan dihapus.\n"
     ]
    }
   ],
   "source": [
    "# fungsi untuk menghapus angka dari teks\n",
    "def remove_numbers(teks):\n",
    "    teks_tanpa_angka = ''.join([char for char in teks if not char.isdigit()])\n",
    "    return teks_tanpa_angka\n",
    "\n",
    "#contoh teks dengan angka\n",
    "teks_dengan_angka = \"Ini adalah contoh teks dengan angka 12345 yang akan dihapus.\"\n",
    "\n",
    "#  Memanggil fungsi untuk menghapus angka\n",
    "teks_tanpa_angka = remove_numbers(teks_dengan_angka)\n",
    "\n",
    "# Menampilkan hasil\n",
    "print(\"Teks dengan angka:\")\n",
    "print(teks_dengan_angka)\n",
    "print(\"\\nTeks tanpa angka:\")\n",
    "print(teks_tanpa_angka)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yFtLhUL-8kv-",
    "outputId": "404f2f04-fe1f-4acf-b992-70b45a8fcb52"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teks dengan angka:\n",
      "Ini adalah contoh teks dengan angka 12345 yang akan dihapus.\n",
      "\n",
      "Teks tanpa angka:\n",
      "Ini adalah contoh teks dengan angka  yang akan dihapus.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def remove_numbers(teks):\n",
    "  #menggunakan regular expression dengan menghapus angka\n",
    "    teks_tanpa_angka = re.sub(r'\\d+', '', teks)\n",
    "    return teks_tanpa_angka\n",
    "\n",
    "#contoh teks dengan angka\n",
    "teks_dengan_angka = \"Ini adalah contoh teks dengan angka 12345 yang akan dihapus.\"\n",
    "\n",
    "#  Memanggil fungsi untuk menghapus angka\n",
    "teks_tanpa_angka = remove_numbers(teks_dengan_angka)\n",
    "\n",
    "# Menampilkan hasil\n",
    "print(\"Teks dengan angka:\")\n",
    "print(teks_dengan_angka)\n",
    "print(\"\\nTeks tanpa angka:\")\n",
    "print(teks_tanpa_angka)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8hOJKMVe83MN",
    "outputId": "07adeb76-7f0c-4746-dbdf-ceb0d9d7ac12"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kalimat dengan angka tidak relevan:\n",
      "Di sini ada beberapa nomor rumah yaitu  123, 456, dan 789. Silakan hubungi 081234567890 untuk informasi lebih lanjut.\n",
      "kalimat tanpa angka tidak relevan:\n",
      "Di sini ada beberapa nomor rumah yaitu  123, 456, dan 789. Silakan hubungi  untuk informasi lebih lanjut.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def hapus_angka_tidak_relevan(teks):\n",
    "  #menggunakan regex untuk mengidentifikasi dan menghapus angka yang tidak relevan\n",
    "  #pola unutk mengenasli angka yang harus di hapus, termasuk nomor rumah dan nomor telepon\n",
    "  pola_angka_tidak_relevan = r\"\\b(?:\\d{1,3}[-\\.\\s]?)?(?:\\d[3][-\\/\\s]?)?\\d{4,}\\b\"\n",
    "  hasil = re.sub(pola_angka_tidak_relevan, '', teks)\n",
    "  return hasil\n",
    "\n",
    "#contoh kalimat dengan angka\n",
    "kalimat = \"Di sini ada beberapa nomor rumah yaitu  123, 456, dan 789. Silakan hubungi 081234567890 untuk informasi lebih lanjut.\"\n",
    "\n",
    "#Memanggil fungsi untuk menghapus angka tidak relevan\n",
    "kalimat_tanpa_angka_tidak_relevan = hapus_angka_tidak_relevan(kalimat)\n",
    "\n",
    "#Menampilkan hasil\n",
    "print(\"Kalimat dengan angka tidak relevan:\")\n",
    "print(kalimat)\n",
    "print(\"kalimat tanpa angka tidak relevan:\")\n",
    "print(kalimat_tanpa_angka_tidak_relevan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vji9fLJj_FPB"
   },
   "source": [
    "## **A.2 Menghapus Tanda Baca**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tSd_94BG_CSU",
    "outputId": "ea3c432a-2a71-457c-e64b-b6cdc0181dd5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teks asli:\n",
      "Ini adalah contoh teks, dengan tanda baca! Contoh ini, digunakan? untuk demonstrasi.\n",
      "\n",
      "Teks tanpa tanda baca:\n",
      "Ini adalah contoh teks dengan tanda baca Contoh ini digunakan untuk demonstrasi\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "def remove_punctuation(text):\n",
    "  #membuat set yang berisi semua tanda baca\n",
    "  puncttuation_set =  set(string.punctuation)\n",
    "\n",
    "  # Menghapus tanda baca dari teks\n",
    "  text_without_punctuation = ''.join(char for char in text if char not in puncttuation_set)\n",
    "\n",
    "  return text_without_punctuation\n",
    "\n",
    "# Contoh teks dengan tanda baca\n",
    "teks_asli = \"Ini adalah contoh teks, dengan tanda baca! Contoh ini, digunakan? untuk demonstrasi.\"\n",
    "\n",
    "# Menghapus tanda baca dari teks\n",
    "teks_tanpa_tanda_baca = remove_punctuation(teks_asli)\n",
    "\n",
    "# Menampilkan hasil\n",
    "print(\"Teks asli:\")\n",
    "print(teks_asli)\n",
    "print(\"\\nTeks tanpa tanda baca:\")\n",
    "print(teks_tanpa_tanda_baca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v4gLjXFNAVj9",
    "outputId": "659fb8cd-9d88-48df-eb95-c3a74646e0cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teks asli:\n",
      "\"\n",
      "Dalam dunia ini, banyak hal terjadi, dari yang kecil hingga yang besar. \n",
      "Kita bisa melihat keindahan, tapi juga kekejaman. Ada harapan, namun juga keputusasaan. \n",
      "Bagaimanapun, hidup terus berjalan, tak peduli apa pun yang terjadi!\n",
      "\n",
      "\n",
      "Teks tanpa tanda baca:\n",
      "\n",
      "Dalam dunia ini banyak hal terjadi dari yang kecil hingga yang besar \n",
      "Kita bisa melihat keindahan tapi juga kekejaman Ada harapan namun juga keputusasaan \n",
      "Bagaimanapun hidup terus berjalan tak peduli apa pun yang terjadi\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "def remove_punctuation(text):\n",
    "  #membuat set yang berisi semua tanda baca\n",
    "  punctuation_set = set(string.punctuation)\n",
    "\n",
    "  # Mengapa tanda baca dari teks\n",
    "  text_without_punctuation = ''.join(char for char in text if char not in punctuation_set)\n",
    "\n",
    "  return text_without_punctuation\n",
    "\n",
    "# Contoh teks dengan banayk tanda baca\n",
    "teks_asli = \"\"\"\"\n",
    "Dalam dunia ini, banyak hal terjadi, dari yang kecil hingga yang besar.\n",
    "Kita bisa melihat keindahan, tapi juga kekejaman. Ada harapan, namun juga keputusasaan.\n",
    "Bagaimanapun, hidup terus berjalan, tak peduli apa pun yang terjadi!\n",
    "\"\"\"\n",
    "\n",
    "# Menghaopus tanda baca dari eks\n",
    "teks_tanpa_tanda_baca = remove_punctuation(teks_asli)\n",
    "\n",
    "# Menampilkan hasil\n",
    "print(\"Teks asli:\")\n",
    "print(teks_asli)\n",
    "print(\"\\nTeks tanpa tanda baca:\")\n",
    "print(teks_tanpa_tanda_baca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zlT5NNPxBqIy",
    "outputId": "d99902d6-ed16-4a25-99dd-09155dfe230d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kalimat asli:\n",
      "Hari ini cuaca sangat cerah! Angin bertiup kencang, dan ada beberapa awan di langit. Bagaimana dengan cuaca besok?\n",
      "\n",
      "Kalimat tanpa tanda baca:\n",
      "Hari ini cuaca sangat cerah Angin bertiup kencang dan ada beberapa awan di langit Bagaimana dengan cuaca besok\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "def hapus_tanda_baca(teks):\n",
    "  teks_tanpa_tanda_baca = teks.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "  return teks_tanpa_tanda_baca\n",
    "\n",
    "#contoh kalimat dengan banyak tanda baca\n",
    "kalimat = \"Hari ini cuaca sangat cerah! Angin bertiup kencang, dan ada beberapa awan di langit. Bagaimana dengan cuaca besok?\"\n",
    "\n",
    "# Memanggil fungsi untuk menghaspu tanda baca\n",
    "kalimat_tanpa_tanda_baca = hapus_tanda_baca(kalimat)\n",
    "\n",
    "# Menampilkan hasil\n",
    "print(\"Kalimat asli:\")\n",
    "print(kalimat)\n",
    "print(\"\\nKalimat tanpa tanda baca:\")\n",
    "print(kalimat_tanpa_tanda_baca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9of4QwA1CQsE"
   },
   "source": [
    "## **A.3 Menghapus white space**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kpFttwg-CgRa"
   },
   "source": [
    "### **A.3.1 Menggunakan strip() untuk Menghapus Whitespace di Awal dan Akhir**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o4HJuUbvCciq",
    "outputId": "6a5c1283-5e7b-47ab-efcf-e80934e67a6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teks asli:      Ini adalah contoh kalimat dengan spasi di awal dan di akhir    \n",
      "Teks tanpa whitespace: Ini adalah contoh kalimat dengan spasi di awal dan di akhir\n"
     ]
    }
   ],
   "source": [
    "teks = \"     Ini adalah contoh kalimat dengan spasi di awal dan di akhir    \"\n",
    "teks_tanpa_whitespace = teks.strip()\n",
    "\n",
    "print(\"Teks asli:\", teks)\n",
    "print(\"Teks tanpa whitespace:\", teks_tanpa_whitespace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gVIqAzsMFA_j"
   },
   "source": [
    "### **A.3.2 Menggunakan replace() untuk Menghapus Whitespace di Awal dan Akhir**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0xk8eyzMFD7s",
    "outputId": "9038ccf6-94cf-49a5-ec85-b7e90b5c86ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teks asli:      Ini adalah contoh kalimat dengan spasi di awal dan di akhir    \n",
      "Teks tanpa whitespace: Iniadalahcontohkalimatdenganspasidiawaldandiakhir\n"
     ]
    }
   ],
   "source": [
    "teks = \"     Ini adalah contoh kalimat dengan spasi di awal dan di akhir    \"\n",
    "teks_tanpa_whitespace = teks.replace(\" \", \"\")\n",
    "\n",
    "print(\"Teks asli:\", teks)\n",
    "print(\"Teks tanpa whitespace:\", teks_tanpa_whitespace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sfP3QgNeFRIo"
   },
   "source": [
    "# **Stopword Removal (Filtering)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CVq3wxctFSyS"
   },
   "source": [
    "### **A.1 Stopwords NLTK (Natural Language Toolkit)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P-_Rh7qaFXPL",
    "outputId": "aec349e8-558f-42cd-9ca5-7aeca34fd673"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teks asli: Perekonomian Indonesia sedang dalam pertumbuhan yang membanggakan\n",
      "Teks tanpa stopword: Perekonomian Indonesia pertumbuhan membanggakan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download corpus stopword bahasa indonesia dari NLTK\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt') # Untuk tokenisasai kata\n",
    "nltk.download('punkt_tab') # Download punkt_tab resource\n",
    "\n",
    "teks = \"Perekonomian Indonesia sedang dalam pertumbuhan yang membanggakan\"\n",
    "\n",
    "# Tokenisasi teks menjadi kata-kata\n",
    "tokens_kata = word_tokenize(teks)\n",
    "\n",
    "# Ambil daftar stopword bahasa indonesia dari nltk\n",
    "stopword_bahasa_indonesia = set(stopwords.words('indonesian'))\n",
    "\n",
    "# Menghapus stopword dari teks\n",
    "teks_tanpa_stopword = [kata for kata in tokens_kata if kata.lower() not in stopword_bahasa_indonesia]\n",
    "# Gabungkan kata-kata penting kembali menjadi teks\n",
    "Gabungkan_teks = \" \".join(teks_tanpa_stopword)\n",
    "\n",
    "print(\"Teks asli:\", teks)\n",
    "print(\"Teks tanpa stopword:\", Gabungkan_teks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SwcwUtqRGgv-",
    "outputId": "1075443a-9588-46f4-932c-9dc1432f7982"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teks asli: Ini adalah contoh tokenisasi kata dalam pemrosesan teks.\n",
      "Tokens kata: ['Ini', 'adalah', 'contoh', 'tokenisasi', 'kata', 'dalam', 'pemrosesan', 'teks', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "teks = \"Ini adalah contoh tokenisasi kata dalam pemrosesan teks.\"\n",
    "tokens_kata = word_tokenize(teks)\n",
    "\n",
    "print(\"Teks asli:\", teks)\n",
    "print(\"Tokens kata:\", tokens_kata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 180,
     "referenced_widgets": [
      "e03dbed3092e4deb89682af423b93544",
      "73bec758c9cf47da9125d29137f09655",
      "8c735bbf3c3b4a90904e31b852db1eb7",
      "8706d22839424cb0b0211f2424b625c3",
      "dcea5b7887334684989572f42bff5cfa",
      "73635dae89024b928aab3b44cc0cd209",
      "cce5276c145545a899f0276abe97f085",
      "bdb46ba71a4b4702800cb395484c08a7",
      "e5cafb79b4234e1a93e7dcaa78119bba",
      "8a46d76ec1ad410d88203f8858390eab",
      "f1972541d4ae4829bf275d5a1e3f29aa",
      "512fbebfd41e414d9f948ef9030f33b7",
      "127960b4c5a247daa9032aa39b1dd3da",
      "479451e0d05f4edebde1d16ed5275467",
      "bd9b68f3b46747839eef77f277c5f680",
      "bfc63040f85245ddb750f1ce230caf75",
      "9038d951812c4a25a96fecb55bdd9b28",
      "54f097ab06554b6fb758d6ed9568033d",
      "c0b84fe26b464dc3a3b6134968d8f0ce",
      "e2e2a53d68344b01a2c891c609077e50",
      "9472010e663047b09c52f796438e45d3",
      "617e7fe8d5e94337842622c2363ee9be",
      "be64e6fbe3c34efd875c9f2ffaa03d20",
      "77779060f60e4f0584c3b5a0fd525f25",
      "c9df781d4430402d8757a048256adf88",
      "050f29e336554d318d14f86e9a7c2ca3",
      "132e88cfda1f490e8587704911dfe128",
      "7f4e14e99c1a4cb599ce175192b59fb5",
      "a22d2d51e0fa4a90978d4c1401f6ae28",
      "cda6d2fdf4194d638661f1579f2b5057",
      "0fb15a41443b43699a85541cbdf7723d",
      "348bfc84c34d4de18c97685ad25c2f1b",
      "6c6c6d18407340fcb34b44d160ccb1d4",
      "434d2b095b194ab8a418dda1c88c5273",
      "a3822a5e8b0c4e79a2b29080a39675d9",
      "9c1fa89c31324abdb6c0ae4265aec31f",
      "7143364db2b0411bb482cd726783d6d7",
      "e6c64e37f14c4c5880dec7b78a91f7d9",
      "5b28fe9fa73347b2891d7a99de9175c5",
      "c79d42d7929c4b97ace12b91ac8c99fc",
      "44007004282b46528339e75ce9583c28",
      "04da40a32ba041bf89b36c967524d07e",
      "c34fc96546b64609b529b61de6da9448",
      "d05a653d9e8946b7a5b20ffd34a000bf"
     ]
    },
    "id": "x5y9PJ45IUkJ",
    "outputId": "6d7b0eb6-bfa6-42f3-ce24-c534e756f8bf"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e03dbed3092e4deb89682af423b93544",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "512fbebfd41e414d9f948ef9030f33b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be64e6fbe3c34efd875c9f2ffaa03d20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "434d2b095b194ab8a418dda1c88c5273",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teks asli: Ini adalah contoh tokenisasi kata dalam pemrosesan teks.\n",
      "Tokens kata: ['in', '##i', 'ada', '##lah', 'con', '##to', '##h', 'token', '##isa', '##si', 'kata', 'dal', '##am', 'pe', '##m', '##rose', '##san', 'te', '##ks', '.']\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "teks = \"Ini adalah contoh tokenisasi kata dalam pemrosesan teks.\"\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "tokens_kata = tokenizer.tokenize(teks)\n",
    "\n",
    "print(\"Teks asli:\", teks)\n",
    "print(\"Tokens kata:\", tokens_kata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s8guD6GmIpPk",
    "outputId": "0c8bfb94-3748-4673-885b-db38f66da8a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kalimat panjang setelah penghapusan stopword: Pengembangan kecerdasan buatan (artificial intelligence AI) fokus utama dunia teknologi. Teknologi AI komputer belajar data, menemukan pola-pola kompleks, keputusan manusia. Penggunaan AI luas bidang pengolahan bahasa alami (NLP), pengenalan wajah, mobil otonom, analisis data, lagi.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Mengimpor library NLTK untuk pengelolaan teks\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    # Mengambil daftar stopword bahasa Indonesia dari NLTK\n",
    "    stop_words_indonesian = set(stopwords.words('indonesian'))\n",
    "\n",
    "    # Memisahkan teks menjadi kata-kata\n",
    "    words = text.split()\n",
    "\n",
    "    # Menghapus stopword dari teks\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words_indonesian]\n",
    "\n",
    "    # Menggabungkan kata-kata yang sudah difilter kembali menjadi kalimat\n",
    "    filtered_text = ' '.join(filtered_words)\n",
    "\n",
    "    return filtered_text\n",
    "\n",
    "# Kalimat panjang sebelum penghapusan stopword\n",
    "kalimat_panjang = \"Pengembangan kecerdasan buatan (artificial intelligence atau AI) telah menjadi fokus utama dalam dunia teknologi. Teknologi AI memungkinkan komputer untuk belajar dari data, menemukan pola-pola kompleks, dan membuat keputusan seperti manusia. Penggunaan AI yang luas terlihat dalam berbagai bidang seperti pengolahan bahasa alami (NLP), pengenalan wajah, mobil otonom, analisis data, dan banyak lagi.\"\n",
    "\n",
    "# Memanggil fungsi untuk menghapus stopword pada kalimat panjang\n",
    "kalimat_panjang_filtered = remove_stopwords(kalimat_panjang)\n",
    "\n",
    "# Output teks setelah penghapusan stopword\n",
    "print(\"Kalimat panjang setelah penghapusan stopword:\", kalimat_panjang_filtered)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mwDSP-oOIups"
   },
   "source": [
    "### **A.1 Stopwords Sastrawi**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pT5y54_sI2fI",
    "outputId": "a0590732-4cad-42b8-a688-c4d37881c12a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Sastrawi in /usr/local/lib/python3.11/dist-packages (1.0.1)\n",
      "Teks asli: Perekonomian Indonesia sedang dalam pertumbuhan yang membanggakan.\n",
      "Teks setelah filtering stopwords Sastrawi: Perekonomian Indonesia sedang pertumbuhan membanggakan .\n"
     ]
    }
   ],
   "source": [
    "# Install Sastrawi library if not already installed\n",
    "!pip install Sastrawi\n",
    "\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Inisialisasi objek StopWordRemover dari Sastrawi\n",
    "factory = StopWordRemoverFactory()\n",
    "stopwords_sastrawi = factory.get_stop_words()\n",
    "\n",
    "teks = \"Perekonomian Indonesia sedang dalam pertumbuhan yang membanggakan.\"\n",
    "\n",
    "# Tokenisasi teks menjadi kata-kata\n",
    "tokens_kata = word_tokenize(teks)\n",
    "\n",
    "# Filtering kata-kata dengan menghapus stopwords Sastrawi\n",
    "kata_penting = [kata for kata in tokens_kata if kata.lower() not in stopwords_sastrawi]\n",
    "\n",
    "# Gabungkan kata-kata penting kembali menjadi teks\n",
    "teks_tanpa_stopwords = ' '.join(kata_penting)\n",
    "\n",
    "print(\"Teks asli:\", teks)\n",
    "print(\"Teks setelah filtering stopwords Sastrawi:\", teks_tanpa_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zbTTRh-9JGgx"
   },
   "source": [
    "# **Tokenizing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "movcC95WJHhA"
   },
   "source": [
    "## **A.1 Tokenisasi Kata (Word Tokenization)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5w-0xcLGJMsT",
    "outputId": "fa49d260-c588-4b90-8da6-5c42e4971c6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ini', 'adalah', 'contoh', 'kalimat', 'untuk', 'tokenisasi', 'kata', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"Ini adalah contoh kalimat untuk tokenisasi kata.\"\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KJWOdeD-JSzh"
   },
   "source": [
    "## **Tokenisasi Kalimat (Sentence Tokenization)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wnm38IduJUC0",
    "outputId": "4bce56e1-776d-456d-b1e8-a589ce841104"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ini adalah contoh kalimat pertama.', 'Dan ini adalah contoh kalimat kedua.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text = \"Ini adalah contoh kalimat pertama. Dan ini adalah contoh kalimat kedua.\"\n",
    "sentences = sent_tokenize(text)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u0bb5Z5iJWCA"
   },
   "source": [
    "## **Tokenisasi Frasa (Phrase Tokenization**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Yb2a5HqTJWye",
    "outputId": "8543d40f-8b98-495a-b643-6d8745506a63"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Apel', ' jeruk', ' pisang', ' dan mangga.']\n"
     ]
    }
   ],
   "source": [
    "# Misalkan kita ingin memisahkan frasa berdasarkan tanda baca koma (,)\n",
    "text = \"Apel, jeruk, pisang, dan mangga.\"\n",
    "phrases = text.split(',')\n",
    "print(phrases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9zCiJlG1Jan2"
   },
   "source": [
    "## **Tokenisasi Berdasarkan Aturan (Rule-based Tokenization)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R1OZ-48uJbZ_",
    "outputId": "54540252-638b-4265-f019-8c97bbdc07be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Pertama', 'kita', 'perlu', 'menyiapkan', 'bahan', 'bahan', 'yang', 'diperlukan']\n"
     ]
    }
   ],
   "source": [
    "# Contoh aturan tokenisasi khusus untuk tokenisasi kata dalam bahasa Indonesia\n",
    "import re\n",
    "\n",
    "text = \"Pertama, kita perlu menyiapkan bahan-bahan yang diperlukan.\"\n",
    "tokens = re.findall(r'\\w+|\\d+', text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s5lyQFg5JfOv"
   },
   "source": [
    "## **Tokenisasi Berdasarkan Model (Model-based Tokenization)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EyoHDDIUJf_N",
    "outputId": "08f97d7a-400c-4c11-ce93-b09ad59f199d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ini', 'adalah', 'contoh', 'tokenisasi', 'berbasis', 'model.']\n"
     ]
    }
   ],
   "source": [
    "# Misalnya menggunakan spasi sebagai pemisah kata\n",
    "text = \"Ini adalah contoh tokenisasi berbasis model.\"\n",
    "tokens = text.split()\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XHw8nltZJk78"
   },
   "source": [
    "# **Stemming**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LlZcKjAHJlsn"
   },
   "source": [
    "## **Stemming menggunakan NLTK**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ex6WMl8mJnzp",
    "outputId": "e9ea77e1-0b09-4172-d995-8d221cdaebed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['run', 'easili', 'bought', 'cri', 'leav']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "words = [\"running\", \"easily\", \"bought\", \"crying\", \"leaves\"]\n",
    "\n",
    "stemmed_words = [stemmer.stem(word) for word in words]\n",
    "print(stemmed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CdKB1a_EJrgy"
   },
   "source": [
    "# **Lemmatization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M8nqnXr7JsLq"
   },
   "source": [
    "## **Lemmatisasi menggunakan NLTK**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7yDQxccaJr8n",
    "outputId": "8b0dcada-3780-4bdc-c185-81b81d901002"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['run', 'easily', 'buy', 'cry', 'leave']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "words = [\"running\", \"easily\", \"bought\", \"crying\", \"leaves\"]\n",
    "\n",
    "lemmatized_words = [lemmatizer.lemmatize(word, pos=wordnet.VERB) for word in words]\n",
    "print(lemmatized_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LHA3mCN7JxqU",
    "outputId": "da678f5b-c218-4cab-b920-41db40a1353d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tokens: ['The', 'cats', 'are', 'running', 'and', 'jumping', 'over', 'the', 'fences', '.']\n",
      "Stemmed tokens: ['the', 'cat', 'are', 'run', 'and', 'jump', 'over', 'the', 'fenc', '.']\n",
      "Lemmatized tokens: ['The', 'cat', 'are', 'running', 'and', 'jumping', 'over', 'the', 'fence', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Kalimat contoh\n",
    "sentence = \"The cats are running and jumping over the fences.\"\n",
    "\n",
    "# Inisialisasi stemmer dan lemmatizer\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Tokenisasi kalimat\n",
    "tokens = word_tokenize(sentence)\n",
    "\n",
    "# Stemming\n",
    "stemmed_words = [stemmer.stem(word) for word in tokens]\n",
    "\n",
    "# Lemmatisasi\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "print(\"Original tokens:\", tokens)\n",
    "print(\"Stemmed tokens:\", stemmed_words)\n",
    "print(\"Lemmatized tokens:\", lemmatized_words)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
